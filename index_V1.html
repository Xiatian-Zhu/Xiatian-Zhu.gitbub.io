<!DOCTYPE html><html lang="en">	<head>		<meta charset="utf-8">		<title>Xiatian Zhu</title>		<meta name="viewport" content="width=device-width, initial-scale=1.0">		<meta name="description" content="Xiatian Zhu">		<meta name="author" content="Xiatian Zhu">				<meta name="keywords" content="Xiatian Zhu, Xia Tian Zhu, X. Zhu, XT Zhu, XTZhu, Xiatian (Eddy) Zhu, Eddy, 			Visual Surveillance, Video Summarisation, Video Summarization, Video Synopsis,			Camera Network, Multi-Camera Correlation, Feature Coding,			Multi-Source, Multisource, Heterogeneous Sources, Source Correlation,			Random Forest, Clustering Random Forest, Constrained Clustering Random Forest,			Constrained Spectral Clustering, Pairwise Constraint Propagation, Noisy Constraints, Imperfect Oracles,			Constraint Propagation Random Forest, COP-RF" />        		<!-- Le styles -->		<link href="./css/bootstrap.css" rel="stylesheet">		<link href="./css/bootstrap-responsive.css" rel="stylesheet">		<link href="./prettify.css" rel="stylesheet">		<link href="./css/docs.css" rel="stylesheet">		<link href="./css/cavan.css" rel="stylesheet">		<link href="./css/Eddy.css" rel="stylesheet">		<!-- HTML5 shim, for IE6-8 support of HTML5 elements                 vd5dN32lqt            -->		<!--[if lt IE 9]>		  <script src="../assets/js/html5shiv.js"></script>		<![endif]-->		<!-- Fav and touch icons -->		<link rel="shortcut icon" href="./images/icon/fav.png">		<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-144-precomposed.png">        <link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-114-precomposed.png">        <link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-72-precomposed.png">        <link rel="apple-touch-icon-precomposed" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-57-precomposed.png">			</head>	<body>			<!-- Navigation ================================================== -->		<div class="navbar navbar-inverse navbar-fixed-top content_width_range">			<div class="navbar-inner">				<div class="container" >				<!--<div class="container-fluid">					<button type="button" class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">						<span class="icon-bar"></span>						<span class="icon-bar"></span>						<span class="icon-bar"></span>					</button>					<a class="brand" href="#">Project name</a>-->					<div class="nav-collapse collapse">						<!--<p class="navbar-text pull-right">							Logged in as <a href="#" class="navbar-link">Username</a>						</p>-->						<ul class="nav">							<li class="active"><a href="#"><strong>Home</strong></a></li>							<li><a href="#PUBLICATION"><strong>Publication</strong></a></li>							<li><a href="download.html"><strong>Download</strong></a></li>						</ul>					</div>				<!--</div>/.nav-collapse-->				</div><!--/.container-->			</div>		</div>				<!-- Profile Image & About Me ================================================== -->		<div class="container">					<div class="container-fluid content_width_range">								<div class="row-fluid basic_info">					<!-- Profile Image and Text -->					<div class="span5">											<!--ul class="nav nav-list" -->						<ul class="">								<!--li-->								<div class="span5" style="text-align: center;">									<img style="width: 100%;" src="./images/res/zxt.png" alt="Xiatian Zhu"/>									<h2 > Xiatian Zhu </h2> 									<!--h4 style="color:gray;">										Ph.D. Student, &nbsp; Student Member IEEE									</h4-->								</div>								<div class="span7" style="padding-top:5px; padding-left: 10px;">									<address>										Computer Vision Group,										<br/>										School of Electronic Engineering and Computer Science,										<br/>										Queen Mary University of London,										<br/>										London E1 4NS, 										<br/> 										United Kingdom.										<br/>										<br/>										Email: xiatian.zhu AT qmul.ac.uk										<!--a href="mailto:xiatian.zhu@qmul.ac.uk">											<img style="width:30px;" src="./images/icon/email.png"/> 											<a href="http://www.google.com/recaptcha/mailhide/d?k=01jJvG6PQq6fxnCkheId9jug==&amp;c=lnm2znfCj-5N1OTbbksizWE74qebStG1GMBx7cWQ48I=" onclick="window.open('http://www.google.com/recaptcha/mailhide/d?k\07501jJvG6PQq6fxnCkheId9jug\75\75\46c\75lnm2znfCj-5N1OTbbksizWE74qebStG1GMBx7cWQ48I\075', '', 'toolbar=0,scrollbars=0,location=0,statusbar=0,menubar=0,resizable=0,width=500,height=300'); return false;" title="Reveal this e-mail address">click to view</a>@qmul.ac.uk 										</a -->									</address>								</div>															<!--/li>														<li>								<div class="page-header"> 								</div>							</li-->						</ul>										</div><!--/span-->										<!-- About Me -->					<div class="span7">						<div class="aboutme">							<!--img style="width: 30%;" src="./images/wordle/wordle.png" alt="wordle"/>							<div class="page-header"> 								<h2> About Me </h2> 								<h3 style="color:LightCoral ">I am in the job market.								</h3>							</div-->														<p> Xiatian Zhu is PhD student (the final year) in the 								<a href="http://vision.eecs.qmul.ac.uk/">Computer Vision Group </a> of 								Queen Mary University of London (QMUL), under the supervision of 								<a href="http://www.eecs.qmul.ac.uk/~sgg/"> Prof. Shaogang Gong</a> and 								<a href="http://www.eecs.qmul.ac.uk/~txiang/">Dr. Tao Xiang</a>.								<!--<a href="http://www.eecs.qmul.ac.uk/~phao/"> Dr. Pengwei Hao</a> (Independent Assessor)-->								Prior to joining QMUL, he received the B.Eng and M.Eng. degree both in Computer Science from 								University of Electronic Science and Technology of China. 								His research interests include computer vision and pattern recognition, 								particularly in visual surveillance, multi-source structure discovery, random forests. 							</p>								His CV is 								[<a href="CV(XiatianZhu).pdf">here</a>].							<!--<p>								<a href="http://scholar.google.com/citations?view_op=list_works&hl=en&user=ZbA-z1cAAAAJ">									<img style="width:20px;" src="./images/icon/link.png"/>									Google Scholar profile								</a>								<a href="http://www.informatik.uni-trier.de/~ley/pers/hy/z/Zhu:Xiatian.html">									<img style="width:20px;" src="./images/icon/link.png"/>									DBLP								</a>							</p>-->							<!--<p><a href="#" class="btn btn-primary btn-large">Learn more &raquo;</a></p>-->																				</div>					</div><!--/span-->				</div><!--/row-->			</div><!--/.fluid-container-->																		<!-- Publications ================================================== -->		<div id="PUBLICATION" />		<div class="page-header content_width_range"> 			<h3> Publication  			<font face="Times New Roman" size="4" color="#999999"></font>			</h3>		</div>				<div class="container-fluid content_width_range paper_block">							<!-- ************************************************************************************* -->			<!-- ECCV 2014 -->			<div class="span4 thumbnail">				<img style="width: 400px;" src="./papers/ECCV14/side_video_ranking_reid.png" alt="ECCV14">			</div>			<div class="span7">				<p>					<img style="width:30px;" src="./images/icon/paper.png"/> &nbsp;					T. Wang, S. Gong, X. Zhu and S. Wang.					<b>Person Re-Identification by Video Ranking</b>.					In Proc. European Conference on Computer Vision, Zurich, Switzerland, September 2014. <b>(ECCV)</b> &nbsp;					<!-- paper download -->					[<a href='./papers/ECCV14/WangEtAl_ECCV14.pdf'>PDF</a>] &nbsp;					<!-- [<a href='downloads_qmul_iLIDS-VID_ReID_dataset.html'>iLIDS Video re-IDentification (iLIDS-VID) Dataset</a>] &nbsp;											[<a href='#'>Poster</a>] &nbsp; -->					[<a href='./project_video_ranking/index.html'>Project page / Dataset / CMC</a>] &nbsp;					<!-- [<a href='#'>Bib</a>] &nbsp; -->									</p> 				<p>					<b>Abstract</b>:					<small>						Current person re-identification (re-id) methods typically rely						on single-frame imagery features, and ignore space-time information from						image sequences. Single-frame (single-shot) visual appearance matching						is inherently limited for person re-id in public spaces due to visual ambiguity						arising from non-overlapping camera views where viewpoint and						lighting changes can cause significant appearance variation. In this work,						we present a novel model to automatically select the most discriminative						video fragments from noisy image sequences of people where more						reliable space-time features can be extracted, whilst simultaneously to						learn a video ranking function for person re-id. Also, we introduce a new						image sequence re-id dataset (iLIDS-VID) based on the i-LIDS MCT						benchmark data. Using the iLIDS-VID and PRID 2011 sequence re-id						datasets, we extensively conducted comparative evaluations to demonstrate						the advantages of the proposed model over contemporary gait						recognition, holistic image sequence matching and state-of-the-art single-shot/						multi-shot based re-id methods.					</small>				</p>							</div>			<img class="separator_line" src="./images/line.png"/><br/>															<!-- ************************************************************************************* -->			<!-- CVPR 2014 -->			<div class="span4 thumbnail">				<img style="width: 400px;" src="./papers/CVPR14/robust_graphs.png" alt="CVPR14">			</div>			<div class="span7">				<p>					<img style="width:30px;" src="./images/icon/paper.png"/> &nbsp;					<!--img style="width:43px;" src="./images/icon/new.png"/-->					<!--<font face="Times New Roman" size="5" color="#DD1111">NEW</font> &nbsp;&nbsp;-->					X. Zhu, C.C. Loy and S. Gong.					<b>Constructing Robust Affinity Graphs for Spectral Clustering</b>.					In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 					Columbus, Ohio, USA, June 2014. <b>(CVPR)</b> &nbsp;					<!-- paper download -->					[<a href='./papers/CVPR14/ZhuLoyGong_CVPR14.pdf'>PDF</a>] &nbsp;										[<a href='./papers/CVPR14/poster_CVPR14.pdf'>Poster</a>] &nbsp;					[<a href='./project_robust_graphs/index.html'>Project page / Codes</a>] &nbsp;					<!-- [<a href='./papers/bib/ZhuLoyGong_CVPR2014.bib'>Bib</a>] &nbsp; -->									</p> 				<p>					<b>Abstract</b>: 					<small>					Spectral clustering requires robust and meaningful affinity graphs as input in order to form clusters with desired structures that can well support human intuition. To construct such affinity graphs is non-trivial due to the ambiguity and uncertainty inherent in the raw data. In contrast to most existing clustering methods that typically employ all available features to construct affinity matrices with the Euclidean distance, which is often not an accurate representation of the underlying data structures, we propose a novel unsupervised approach to generating more robust affinity graphs via identifying and exploiting discriminative features for improving spectral clustering. Specifically, our model is capable of capturing and combining subtle similarity information distributed over discriminative feature subspaces for more accurately revealing the latent data distribution and thereby leading to improved data clustering, especially with heterogeneous data sources. We demonstrate the efficacy of the proposed approach on challenging image and video datasets.					</small>				</p>								<!--<p><a class="btn" href="#">View details &raquo;</a></p>-->			</div>			<img class="separator_line" src="./images/line.png"/><br/>					<!-- ************************************************************************************* -->			<!-- ICCV2013, IJCV2015 -->			<div class="span4 thumbnail">				<img style="width: 400px;" src="./papers/ICCV13/slide_video_synopsis.png" alt="ICCV2013">			</div>			<div class="span7">				<p>					<img style="width:30px;" src="./images/icon/paper.png"/> &nbsp;					<!--<font face="Times New Roman" size="5" color="#DD1111">NEW</font> &nbsp;&nbsp;-->					X. Zhu, C.C. Loy and S. Gong.					<b>Learning from Multiple Sources for Video Summarisation</b>.					International Journal of Computer Vision, in print, 2015. <b>(IJCV)</b> 					[<a href='./papers/ZhuLoyGong_IJCV2015.pdf'>Preprint</a>] &nbsp;					[<a href='./project_video_synopsis/index.html'>Project page</a>] &nbsp;					<br/> <br/>					<img style="width:30px;" src="./images/icon/paper.png"/> &nbsp;					<!--<font face="Times New Roman" size="5" color="#DD1111">NEW</font> &nbsp;&nbsp;-->					X. Zhu, C.C. Loy and S. Gong.					<b>Video Synopsis by Heterogeneous Multi-Source Correlation</b>.					In Proc. IEEE International Conference on Computer Vision, 					Sydney, Australia, December 2013. <b>(ICCV)</b> &nbsp;					<!-- paper download -->					[<a href='./papers/ICCV13/ZhuLoyGong_ICCV2013.pdf'>PDF</a>] &nbsp;										[<a href='./papers/ICCV13/Poster_ICCV_2013.pdf'>Poster</a>] &nbsp;					[<a href='./project_video_synopsis/index.html'>Project page</a>] &nbsp;					<!-- [<a href='./papers/bib/ZhuLoyGong_ICCV2013.bib'>Bib</a>] &nbsp; -->												</p> 												<p>					<b>Abstract</b>: 					<small>					Generating coherent synopsis for surveillance video					stream remains a formidable challenge due to the ambiguity					and uncertainty inherent to visual observations. In					contrast to existing video synopsis approaches that rely on					visual cues alone, we propose a novel multi-source synopsis					framework capable of correlating visual data and independent					non-visual auxiliary information to better describe and					summarise subtle physical events in complex scenes. Specifically,					our unsupervised framework is capable of seamlessly					uncovering latent correlations among heterogeneous types					of data sources, despite the non-trivial heteroscedasticity					and dimensionality discrepancy problems. Additionally, the					proposed model is robust to partial or missing non-visual					information. We demonstrate the effectiveness of our framework					on two crowded public surveillance datasets.					</small>				</p>				<!--<p><a class="btn" href="#">View details &raquo;</a></p>-->			</div>			<img class="separator_line" src="./images/line.png"/><br/>			<!-- ************************************************************************************* -->			<!-- ICDM 2013, TNNLS2015 -->			<div class="span4 thumbnail">				<img style="width: 400px;" src="./papers/ICDM13/slide_constrained_clustering.png" alt="ICDM2013">			</div>			<div class="span7">				<p>					<img style="width:30px;" src="./images/icon/paper.png"/> &nbsp;					X. Zhu, C.C. Loy and S. Gong.					<b>Constrained Clustering with Imperfect Oracles</b>.										IEEE Transactions on Neural Networks and Learning Systems, 2015 <b>(TNNLS)</b>					<!-- paper download -->										[<a href='http://dx.doi.org/10.1109/TNNLS.2014.2387425'>DOI</a>] &nbsp;					[<a href='./papers/TNNLS15/ZhuLoyGong_TNNLS2015.pdf'>PDF</a>] &nbsp;					[<a href='./project_constrained_clustering/index.html'>Project page</a>] &nbsp;					<!-- [<a href='./papers/bib/ZhuLoyGong_TNNLS2015.bib'>Bib</a>] &nbsp; -->					<br/> <br/>					<img style="width:30px;" src="./images/icon/paper.png"/> &nbsp;					X. Zhu, C.C. Loy and S. Gong.					<b>Constrained Clustering: Effective Constraint Propagation with Imperfect Oracles</b>.					In Proc. IEEE International Conference on Data Mining,					Dallas, Texas, USA, December 2013. <b>(ICDM)</b> &nbsp;										<!-- paper download -->					[<a href='./papers/ICDM13/ZhuLoyGong_ICDM2013.pdf'>PDF</a>] &nbsp;					[<a href='./project_constrained_clustering/index.html'>Project page</a>] &nbsp;					<!-- [<a href='./papers/bib/ZhuLoyGong_ICDM2013.bib'>Bib</a>] &nbsp; -->														</p>								<p>					<b>Abstract</b>: 					<small>					While spectral clustering is usually an unsupervised					operation, there are circumstances in which we have prior					belief that pairs of samples should (or should not) be assigned					with the same cluster. Constrained spectral clustering aims to					exploit this prior belief as constraint (or weak supervision) to					influence the cluster formation so as to obtain a structure more					closely resembling human perception. Two important issues remain					open: (1) how to propagate sparse constraints effectively,					(2) how to handle ill-conditioned/noisy constraints generated by					imperfect oracles. In this paper we present a unified framework					to address the above issues. Specifically, in contrast to existing					constrained spectral clustering approaches that blindly rely on					all features for constructing the spectral, our approach searches					for neighbours driven by discriminative feature selection for					more effective constraint diffusion. Crucially, we formulate					a novel data-driven filtering approach to handle the noisy					constraint problem, which has been unrealistically ignored in					constrained spectral clustering literature.					</small>				</p>				<!--<p><a class="btn" href="#">View details &raquo;</a></p>-->							</div><!--/span-->						<img class="separator_line" src="./images/line.png"/>																																<!-- ************************************************************************************* -->			<!-- BMVC 2013 -->						<div class="span4 thumbnail">				<img style="width: 400px;" src="./papers/BMVC12/slide_camera_correlation.png" alt="BMVC2012">			</div>			<div class="span7">				<p>					<img style="width:30px;" src="./images/icon/paper.png"/> &nbsp; &nbsp;					X. Zhu, S. Gong and C.C. Loy.					<b> 						Comparing Visual Feature Coding for Learning Disjoint Camera Dependencies.					</b>					In Proc. British Machine Vision Conference, Guildford, UK, September 2012. <b>(BMVC)</b>					&nbsp;					<!-- paper download -->					[<a href='./papers/BMVC12/ZhuGongLoy_BMVC2012.pdf'>PDF</a>] &nbsp;					[<a href='./papers/BMVC12/BMVC2012_Abstract.pdf'> Extended abstract </a>] &nbsp;					[<a href='./papers/BMVC12/BMVC2012_poster.pdf'>Poster</a>] &nbsp;					[<a href='./papers/BMVC12/feature_coding.zip'>Code</a>] &nbsp;					<!-- [<a href='./papers/bib/ZhuGongLoy_BMVC2012.bib'>Bib</a>] &nbsp; -->				</p>								<p>					<b>Abstract</b>: 					<small>					This paper systematically investigates the effectiveness of different visual feature					coding schemes for facilitating the learning of time-delayed dependencies among disjoint					multi-camera views. Accurate inter-camera dependency estimation across nonoverlapping					camera views is non-trivial especially in crowded scenes where inter-object					occlusion can be severe and frequent, and when the degree of crowdedness can change					drastically over time. In contrast to existing methods that learn dependencies between					disjoint cameras by solely relying on correlating universal object-independent low-level					visual features or transition time statistics, we propose to use either supervised or unsupervised					feature coding, to establish a robust and reliable representation for estimating					more accurately inter-camera activity pattern dependencies. We show comparative experiments					to demonstrate the superiority of robust feature coding for learning inter-camera					dependencies using benchmark multi-camera datasets of crowded public scenes.					</small>				</p>							</div><!--/span-->			<img class="separator_line" src="./images/line.png"/>					</div><!--/row-->												<!--div id="AWARDS" /-->		<div class="page-header content_width_range"> 			<h3> Awards  			<font face="Times New Roman" size="4" color="#999999"></font>			</h3>		</div>				<div class="container-fluid content_width_range paper_block">			<ul>			<li>ECCV Student Travel Grant, 2014</li>			<li>ICCV Student Travel Grant (sponsored by PAMI-TC), 2013</li>			<li>BMVA International Conference Travel Bursary, 2013</li>			<li>QMUL Postgraduate Research Fund, 2013</li>			</ul>						<div class="pull-right">				<a href="#">Back to top</a>			</div>		</div>		<!-- Footer ================================================== -->		<footer class="footer">			<!--<div class="page-header"> </div>-->			<p class="pull-right">				Updated July 2014				<br/>				Created using <a href="http://getbootstrap.com/" target="_blank">bootstrap</a>			</p>					<div id="clustrmaps-widget" class="pull-left"></div><script type="text/javascript">var _clustrmaps = {'url' : 'http://www.eecs.qmul.ac.uk/~xz303/', 'user' : 1113585, 'server' : '2', 'id' : 'clustrmaps-widget', 'version' : 1, 'date' : '2013-09-21', 'lang' : 'en', 'corners' : 'square' };(function (){ var s = document.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'http://www2.clustrmaps.com/counter/map.js'; var x = document.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x);})();</script><noscript><a href="http://www2.clustrmaps.com/user/55710fdf1"><img src="http://www2.clustrmaps.com/stats/maps-no_clusters/www.eecs.qmul.ac.uk-~xz303--thumb.jpg" alt="Locations of visitors to this page" /></a></noscript>							</footer>		</div><!--/container-->						<!-- Le javascript		================================================== -->		<!-- Placed at the end of the document so the pages load faster 		<script src="./js/jquery.js"></script>		<script src="./js/bootstrap-transition.js"></script>		<script src="./js/bootstrap-alert.js"></script>		<script src="./js/bootstrap-modal.js"></script>		<script src="./js/bootstrap-dropdown.js"></script>		<script src="./js/bootstrap-scrollspy.js"></script>		<script src="./js/bootstrap-tab.js"></script>		<script src="./js/bootstrap-tooltip.js"></script>		<script src="./js/bootstrap-popover.js"></script>		<script src="./js/bootstrap-button.js"></script>		<script src="./js/bootstrap-collapse.js"></script>		<script src="./js/bootstrap-carousel.js"></script>		<script src="./js/bootstrap-typeahead.js"></script>-->	</body></html>